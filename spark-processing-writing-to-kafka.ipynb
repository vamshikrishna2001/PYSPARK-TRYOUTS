{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4616c92c-4024-4b40-891d-22af6f11e2da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf()\n",
    "    # print(spark_config['spark.sql.catalogImplementation'])\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(\"spark.master\", \"Local[*]\") \\\n",
    "        .config(\"spark.app.name\",\"modda\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\" , 'hive') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    df = spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"/FileStore/tables/taxi_fare.csv\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS TAXI_DRIVER\")\n",
    "    spark.catalog.setCurrentDatabase(\"TAXI_DRIVER\")\n",
    "    df = df.repartition(5)\n",
    "    df.write.format(\"csv\").mode(\"overwrite\").saveAsTable(\"MARTIN_SCORESCESE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54adc86e-323e-49d9-b60a-7e4cd48320c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='dbfs:/user/hive/warehouse'), Database(name='taxi_driver', catalog='spark_catalog', description='', locationUri='dbfs:/user/hive/warehouse/taxi_driver.db')]\n",
      "[Table(name='martion_scorescese', catalog='spark_catalog', namespace=['taxi_driver'], description=None, tableType='MANAGED', isTemporary=False)]\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id3003155|        2|2016-02-08 07:31:29|2016-02-08 07:40:27|              6| -73.9738540649414| 40.78425598144531|-73.97533416748047| 40.76112747192383|                 N|          538|\n",
      "|id3381536|        2|2016-04-24 20:39:30|2016-04-24 20:50:19|              1|-73.97505950927734|  40.7561149597168|-73.99031829833984| 40.76075744628906|                 N|          649|\n",
      "|id3998901|        2|2016-04-03 08:53:44|2016-04-03 09:04:04|              2|-73.98722076416016|40.776058197021484|-73.96379852294922| 40.80778884887695|                 N|          620|\n",
      "|id0105571|        2|2016-03-10 15:59:09|2016-03-10 16:17:50|              6|-73.97254943847656| 40.75312042236328|-73.98135375976562|40.757808685302734|                 N|         1121|\n",
      "|id3377214|        1|2016-04-04 23:34:33|2016-04-04 23:42:09|              1|-73.99234771728516| 40.73783874511719|-73.99484252929688|40.723358154296875|                 N|          456|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+---------+--------------------+\n",
      "|      key|               value|\n",
      "+---------+--------------------+\n",
      "|id3003155|{\"id\":\"id3003155\"...|\n",
      "|id3381536|{\"id\":\"id3381536\"...|\n",
      "|id3998901|{\"id\":\"id3998901\"...|\n",
      "|id0105571|{\"id\":\"id0105571\"...|\n",
      "|id3377214|{\"id\":\"id3377214\"...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf()\n",
    "    # print(spark_config['spark.sql.catalogImplementation'])\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(\"spark.master\", \"Local[*]\") \\\n",
    "        .config(\"spark.app.name\", \"modda_1\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\" , 'hive') \\\n",
    "        .config(\"spark.jars.packages\" , \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    print(spark.catalog.listDatabases())\n",
    "    print(spark.catalog.listTables(\"taxi_driver\"))    \n",
    "    \n",
    "    table_df = spark.read.table(\"taxi_driver.MARTION_SCORESCESE\")\n",
    "    print(table_df.show(5))\n",
    "    print(table_df.printSchema())\n",
    "    \n",
    "    \n",
    "    df_key_value = table_df.select(f.col('id').alias('key'),f.to_json(f.struct(\"*\")).alias('value'))\n",
    "    print(df_key_value.show(5))\n",
    "    \n",
    "    api_secret = \"\"\n",
    "    api_key = \"\"\n",
    "    # df_key_value.write \\\n",
    "    #     .format(\"kafka\") \\\n",
    "    #     .option(\"kafka.bootstrap.servers\", kafka_conf[\"bootstrap.servers\"]) \\\n",
    "    #     .option(\"topic\", kafka_conf[\"kafka.topic\"]) \\\n",
    "    #     .option(\"kafka.security.protocol\", kafka_conf[\"security.protocol\"]) \\\n",
    "    #     .option(\"kafka.sasl.jaas.config\", kafka_conf[\"kafka.sasl.jaas.config\"].format(api_key, api_secret)) \\\n",
    "    #     .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    #     .option(\"kafka.client.dns.lookup\", kafka_conf[\"kafka.client.dns.lookup\"]) \\\n",
    "    #     .save()\n",
    "    df_key_value.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"pkc-9q8rv.ap-south-2.aws.confluent.cloud:9092\") \\\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "        .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(api_key, api_secret)) \\\n",
    "        .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\") \\\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "        .option(\"topic\",\"Danaerys\") \\\n",
    "        .save()\n",
    "    # df_key_value.write \\\n",
    "    #     .format(\"kafka\") \\\n",
    "    #     .option(\"kafka.bootstrap.servers\", \"localhost:9092,localhost:9093,localhost:9094\") \\\n",
    "    #     .option(\"topic\", \"Targaryen\") \\\n",
    "    #     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b9b389-42c6-4b11-b402-4e96adb349ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-processing-writing-to-kafka",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
